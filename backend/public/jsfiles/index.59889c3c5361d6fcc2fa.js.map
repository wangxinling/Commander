{"version":3,"file":"index.59889c3c5361d6fcc2fa.js","mappings":";;;;;;;;;AAAA;;;;;;SCAA;SACA;;SAEA;SACA;SACA;SACA;SACA;SACA;SACA;SACA;SACA;SACA;SACA;SACA;SACA;;SAEA;SACA;;SAEA;SACA;SACA;;;;;UCtBA;UACA;UACA;UACA,uDAAuD,iBAAiB;UACxE;UACA,gDAAgD,aAAa;UAC7D;;;;;;;;;;;;ACN0D;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,wBAAwB,2EAAwB;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,mEAAgB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,qBAAqB;AAC7C,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,oBAAoB;AAC5C,4BAA4B,wBAAwB;AACpD,kBAAkB,6CAA6C;AAC/D,mBAAmB,4CAA4C;AAC/D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,4CAA4C;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA,CAAC","sources":["webpack://localchat/external module \"https://esm.run/@mlc-ai/web-llm\"","webpack://localchat/webpack/bootstrap","webpack://localchat/webpack/runtime/make namespace object","webpack://localchat/./src/index.js"],"sourcesContent":["module.exports = __WEBPACK_EXTERNAL_MODULE_https_esm_run_mlc_ai_web_llm_c4b7b6a0__;","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","import * as webllm from \"https://esm.run/@mlc-ai/web-llm\";\r\n\r\n/*************** WebLLM logic ***************/\r\nconst messages = [\r\n  {\r\n    content: \"You are a helpful AI agent helping users.\",\r\n    role: \"system\",\r\n  },\r\n];\r\n\r\nconst availableModels = webllm.prebuiltAppConfig.model_list.map(\r\n  (m) => m.model_id,\r\n);\r\nlet selectedModel = \"Llama-3.1-8B-Instruct-q4f32_1-1k\";\r\n\r\n// Callback function for initializing progress\r\nfunction updateEngineInitProgressCallback(report) {\r\n  console.log(\"initialize\", report.progress);\r\n  document.getElementById(\"download-status\").textContent = report.text;\r\n}\r\n\r\n// Create engine instance\r\nconst engine = new webllm.MLCEngine();\r\nengine.setInitProgressCallback(updateEngineInitProgressCallback);\r\n\r\nasync function initializeWebLLMEngine() {\r\n  document.getElementById(\"download-status\").classList.remove(\"hidden\");\r\n  selectedModel = document.getElementById(\"model-selection\").value;\r\n  const config = {\r\n    temperature: 1.0,\r\n    top_p: 1,\r\n  };\r\n  await engine.reload(selectedModel, config);\r\n}\r\n\r\nasync function streamingGenerating(messages, onUpdate, onFinish, onError) {\r\n  try {\r\n    let curMessage = \"\";\r\n    let usage;\r\n    const completion = await engine.chat.completions.create({\r\n      stream: true,\r\n      messages,\r\n      stream_options: { include_usage: true },\r\n    });\r\n    for await (const chunk of completion) {\r\n      const curDelta = chunk.choices[0]?.delta.content;\r\n      if (curDelta) {\r\n        curMessage += curDelta;\r\n      }\r\n      if (chunk.usage) {\r\n        usage = chunk.usage;\r\n      }\r\n      onUpdate(curMessage);\r\n    }\r\n    const finalMessage = await engine.getMessage();\r\n    onFinish(finalMessage, usage);\r\n  } catch (err) {\r\n    onError(err);\r\n  }\r\n}\r\n\r\n/*************** UI logic ***************/\r\nfunction onMessageSend() {\r\n  const input = document.getElementById(\"user-input\").value.trim();\r\n  const message = {\r\n    content: input,\r\n    role: \"user\",\r\n  };\r\n  if (input.length === 0) {\r\n    return;\r\n  }\r\n  document.getElementById(\"send\").disabled = true;\r\n\r\n  messages.push(message);\r\n  appendMessage(message);\r\n\r\n  document.getElementById(\"user-input\").value = \"\";\r\n  document\r\n    .getElementById(\"user-input\")\r\n    .setAttribute(\"placeholder\", \"Generating...\");\r\n\r\n  const aiMessage = {\r\n    content: \"typing...\",\r\n    role: \"assistant\",\r\n  };\r\n  appendMessage(aiMessage);\r\n\r\n  const onFinishGenerating = (finalMessage, usage) => {\r\n    updateLastMessage(finalMessage);\r\n    document.getElementById(\"send\").disabled = false;\r\n    const usageText =\r\n      `prompt_tokens: ${usage.prompt_tokens}, ` +\r\n      `completion_tokens: ${usage.completion_tokens}, ` +\r\n      `prefill: ${usage.extra.prefill_tokens_per_s.toFixed(4)} tokens/sec, ` +\r\n      `decoding: ${usage.extra.decode_tokens_per_s.toFixed(4)} tokens/sec`;\r\n    document.getElementById(\"chat-stats\").classList.remove(\"hidden\");\r\n    document.getElementById(\"chat-stats\").textContent = usageText;\r\n  };\r\n\r\n  streamingGenerating(\r\n    messages,\r\n    updateLastMessage,\r\n    onFinishGenerating,\r\n    console.error,\r\n  );\r\n}\r\n\r\nfunction appendMessage(message) {\r\n  const chatBox = document.getElementById(\"chat-box\");\r\n  const container = document.createElement(\"div\");\r\n  container.classList.add(\"message-container\");\r\n  const newMessage = document.createElement(\"div\");\r\n  newMessage.classList.add(\"message\");\r\n  newMessage.textContent = message.content;\r\n\r\n  if (message.role === \"user\") {\r\n    container.classList.add(\"user\");\r\n  } else {\r\n    container.classList.add(\"assistant\");\r\n  }\r\n\r\n  container.appendChild(newMessage);\r\n  chatBox.appendChild(container);\r\n  chatBox.scrollTop = chatBox.scrollHeight; // Scroll to the latest message\r\n}\r\n\r\nfunction updateLastMessage(content) {\r\n  const messageDoms = document\r\n    .getElementById(\"chat-box\")\r\n    .querySelectorAll(\".message\");\r\n  const lastMessageDom = messageDoms[messageDoms.length - 1];\r\n  lastMessageDom.textContent = content;\r\n}\r\n\r\n/*************** UI binding ***************/\r\navailableModels.forEach((modelId) => {\r\n  const option = document.createElement(\"option\");\r\n  option.value = modelId;\r\n  option.textContent = modelId;\r\n  document.getElementById(\"model-selection\").appendChild(option);\r\n});\r\ndocument.getElementById(\"model-selection\").value = selectedModel;\r\ndocument.getElementById(\"download\").addEventListener(\"click\", function () {\r\n  initializeWebLLMEngine().then(() => {\r\n    document.getElementById(\"send\").disabled = false;\r\n  });\r\n});\r\ndocument.getElementById(\"send\").addEventListener(\"click\", function () {\r\n  onMessageSend();\r\n});\r\n"],"names":[],"sourceRoot":""}